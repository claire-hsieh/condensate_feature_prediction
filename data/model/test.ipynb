{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9967a4a",
   "metadata": {},
   "source": [
    "# Condensate Feature Prediction -- Rotation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d94178",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5b21eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, DataLoader, Subset\n",
    "import psutil\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetTemperature, NVML_TEMPERATURE_GPU\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/z_zb6snx1n95mjp1hndhb_km0000gn/T/ipykernel_9159/3972483665.py:7: DtypeWarning: Columns (6,11,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  large_pl_data = pd.read_csv(\"/Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli kappel/data/large_pool_data_with_seq_info_202507.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{66}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_pl_data = pd.read_csv(\"/Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli_kappel/data/small_pool_data_with_seq_info_202507.csv\", \n",
    "                             index_col=False)\n",
    "list(small_pl_data.columns)\n",
    "small_pl_data.shape\n",
    "\n",
    "# large pool data\n",
    "large_pl_data = pd.read_csv(\"/Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli_kappel/data/large_pool_data_with_seq_info_202507.csv\")\n",
    "large_pl_data.shape\n",
    "list(large_pl_data.columns)\n",
    "\n",
    "# check all are the same length\n",
    "lengths_sml = [len(small_pl_data[\"protein_seq\"][i]) for i in range(small_pl_data.shape[0])]\n",
    "set(lengths_sml)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c7d90",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108e80a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6994, 66, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode protein seq\n",
    "import numpy as np\n",
    "\n",
    "# 20 standard amino acids (IUPAC one-letter codes)\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    # shape: (length_of_seq, 20)\n",
    "    encoding = np.zeros((len(seq), len(amino_acids)), dtype=np.int8)\n",
    "    for i, aa in enumerate(seq):\n",
    "        if aa in aa_to_index:  # skip if non-standard residue\n",
    "            encoding[i, aa_to_index[aa]] = 1\n",
    "    return encoding\n",
    "\n",
    "# # Example\n",
    "# seq = \"MTEYK\"\n",
    "# one_hot = one_hot_encode(seq)\n",
    "# print(one_hot.shape)   # (5, 20)\n",
    "# print(one_hot)\n",
    "\n",
    "subset_data = large_pl_data[[\"protein_seq\", \"medium_GFP_fraction_cells_with_condensates\"]]\n",
    "subset_data = subset_data.dropna(subset=[subset_data.columns[1]])\n",
    "\n",
    "seq_data = np.array([one_hot_encode(i) for i in subset_data[\"protein_seq\"]])\n",
    "labels = subset_data[\"medium_GFP_fraction_cells_with_condensates\"]\n",
    "seq_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all seqs to fasta file\n",
    "large_pl_data = large_pl_data.rename(columns={large_pl_data.columns[0]: \"id\"})\n",
    "\n",
    "output_file = \"/Users/clairehsieh/Library/CloudStorage/OneDrive-Personal/Documents/UCLA/rotations/kalli_kappel/data/large_protein.fasta\"\n",
    "\n",
    "with open(output_file, \"w\") as w:\n",
    "    for i, row in large_pl_data.iterrows():\n",
    "        w.write(f\">{row['id']}\\n\")\n",
    "        w.write(f\"{row['protein_seq']}\\n\")\n",
    "    \n",
    "# run mmseqs2\n",
    "# mmseqs easy-cluster /Users/clairehsieh/Library/CloudStorage/OneDrive-Personal/Documents/UCLA/rotations/kalli_kappel/data/large_protein.fasta clusterRes tmp --min-seq-id 0.9 -c 0.9 --cov-mode 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e1659",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023492ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                 output_dir,\n",
    "                 seed=0,\n",
    "                 num_folds=5,\n",
    "                 max_epochs=100,\n",
    "                 lr=0.001,\n",
    "                 batch_size=10,\n",
    "                 input_dim=100,\n",
    "                 hidden_dim=512,\n",
    "                 output_dim=1,\n",
    "                 num_epochs=500,\n",
    "                 num_layers=3,\n",
    "                 verbose=False,\n",
    "                 layernorm=False,\n",
    "                 dropout=False,\n",
    "                 dropout_rate=0.05, \n",
    "                 early_stopping=True,\n",
    "                 patience = 30,\n",
    "                 delta = 0.05):       \n",
    "        self.layernorm = layernorm\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")       \n",
    "            \n",
    "        # training parameters\n",
    "        self.seed = seed\n",
    "        self.num_folds = num_folds\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_layers = num_layers\n",
    "        self.verbose = verbose\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience    \n",
    "        self.delta = delta\n",
    "        self.output_dir = f\"{output_dir}/seed_{seed}/lr_{lr}/hidden_dim_{hidden_dim}/num_layers_{num_layers}/batch_size_{batch_size}/\"\n",
    "        if not os.path.exists(self.output_dir): os.makedirs(self.output_dir)\n",
    "        if dropout: self.output_dir += f\"dropout_{dropout_rate}/\"\n",
    "        if layernorm: self.output_dir += f\"layernorm/\"\n",
    "        if early_stopping: self.output_dir += f\"early_stopping_patience_{patience}_delta_{delta}/\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(f'{k}: {v}' for k, v in vars(self).items())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.num_layers = config.num_layers\n",
    "        self.input_dim = config.input_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim , self.hidden_dim),\n",
    "            nn.LeakyReLU()\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # final layer always outputs 1 value per sample\n",
    "        self.final = nn.Linear(self.hidden_dim, 1).to(config.device)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_dim).to(config.device)\n",
    "        self.logZ = nn.Parameter(torch.ones(1, device=config.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.mlp(x)        # [batch_size, hidden_dim]\n",
    "        output = self.final(output) # [batch_size, 1]\n",
    "        return output.squeeze(-1)   # [batch_size]\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24c07bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: (6994, 66, 20)\n",
    "#       (batch, seq_len, amino acid)\n",
    "# linearize: (6994, 1320)\n",
    "\n",
    "\n",
    "config = Config(output_dir = \"/Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli kappel/results\",\n",
    "                batch_size=10, input_dim=1320, output_dim=1)\n",
    "\n",
    "class CondensateDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # x: 2D array-like [n_samples, n_features]\n",
    "        # y: 1D array-like [n_samples]\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "#### UTILS ##### \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "def log_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory Usage: {mem_info.rss / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "def mean(ls):\n",
    "    return sum(ls) / len(ls)\n",
    "\n",
    "def compute_spearman_correlation(actuals, predictions):\n",
    "    try:\n",
    "        if np.all(actuals == actuals[0]) or np.all(predictions == predictions[0]):\n",
    "            print(\"Warning: Constant array detected - correlation undefined\")\n",
    "            return 0          \n",
    "        correlation, _ = spearmanr(actuals, predictions)\n",
    "        return correlation\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing correlation: {e}\")\n",
    "        return None\n",
    "\n",
    "def compute_pearson_correlation(actuals, predictions):\n",
    "    if isinstance(actuals, torch.Tensor):\n",
    "        actuals = actuals.cpu().numpy()\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    correlation, _ = pearsonr(actuals, predictions)\n",
    "    return correlation\n",
    "\n",
    "def plot_epoch_losses(epoch_train_losses, epoch_val_losses, filename, output_dir):\n",
    "    print(f\"Train Losses: {epoch_train_losses}\")\n",
    "    print(f\"Val Losses: {epoch_val_losses}\")\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_train_losses, label='Training Loss')\n",
    "    plt.plot(epoch_val_losses, label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss | Fold {filename}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(output_dir + f\"Fold{filename}_loss_plot.jpg\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_fold_scatter(val_actual, val_pred, train_actual, train_pred, correlations, fold, output_dir):   \n",
    "    # Plot scatterplot for validation data\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(val_actual, val_pred, alpha=0.6, label=\"Validation Data\", color=\"orange\")\n",
    "    plt.plot([min(val_actual), max(val_actual)], [min(val_actual), max(val_actual)], 'r--', label=\"Ideal Fit\")\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Validation Data: Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "    # Add correlations as text on the validation plot\n",
    "    plt.text(\n",
    "        0.95, 0.05,\n",
    "        f\"Spearman: {correlations['val_spearman']:.2f}\\nPearson: {correlations['val_pearson']:.2f}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='right',\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "    )\n",
    "    plt.savefig(os.path.join(output_dir, f\"validation_fold_{fold}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot scatterplot for training data\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(train_actual, train_pred, alpha=0.6, label=\"Training Data\", color=\"blue\")\n",
    "    plt.plot([min(train_actual), max(train_actual)], [min(train_actual), max(train_actual)], 'r--', label=\"Ideal Fit\")\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Training Data: Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "     # Add correlations as text on the training plot\n",
    "    plt.text(\n",
    "        0.95, 0.05,\n",
    "        f\"Spearman: {correlations['train_spearman']:.2f}\\nPearson: {correlations['train_pearson']:.2f}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "    )\n",
    "    plt.savefig(os.path.join(output_dir, f\"training_fold_{fold}.jpg\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_all_folds_losses(losses, output_dir, filename=\"all_folds\"):\n",
    "    \"\"\"\n",
    "    losses: dict with keys 'train_losses' and 'val_losses'\n",
    "            values are lists of lists, one list per fold\n",
    "            e.g. losses['train_losses'][i] = list of training losses for fold i\n",
    "    output_dir: directory to save plot\n",
    "    filename: filename prefix for saved plot\n",
    "    \"\"\"\n",
    "    train_folds = losses[\"train_losses\"]\n",
    "    val_folds = losses[\"val_losses\"]\n",
    "\n",
    "    n_folds = len(train_folds)\n",
    "    fig, axes = plt.subplots(1, n_folds, figsize=(5 * n_folds, 4), sharey=True)\n",
    "\n",
    "    if n_folds == 1:  # make axes iterable if only one subplot\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.plot(train_folds[i], label=\"Training Loss\")\n",
    "        ax.plot(val_folds[i], label=\"Validation Loss\")\n",
    "        ax.set_title(f\"Fold {i+1}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{filename}_loss_plot.jpg\"))\n",
    "    plt.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_all_folds_scatter(fold_outputs, all_correlations, output_dir, filename=\"all_folds_scatter\"):\n",
    "    \"\"\"\n",
    "    fold_outputs: dict with keys\n",
    "        'fold_train_actual', 'fold_train_pred',\n",
    "        'fold_val_actual', 'fold_val_pred'\n",
    "        Each is a list of lists (one per fold).\n",
    "\n",
    "    all_correlations: list of dicts (one per fold), each dict has keys\n",
    "        'train_spearman', 'val_spearman', 'train_pearson', 'val_pearson'\n",
    "\n",
    "    output_dir: directory to save the figure\n",
    "    filename: filename prefix for saved plot\n",
    "    \"\"\"\n",
    "\n",
    "    n_folds = len(fold_outputs[\"fold_train_actual\"])\n",
    "    fig, axes = plt.subplots(2, n_folds, figsize=(5 * n_folds, 10), sharey=True)\n",
    "\n",
    "    if n_folds == 1:  # axes will be 1D\n",
    "        axes = axes[:, None]  # make it 2x1 array\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        # Training subplot (top row)\n",
    "        ax_train = axes[0, i]\n",
    "        train_actual = fold_outputs[\"fold_train_actual\"][i]\n",
    "        train_pred = fold_outputs[\"fold_train_pred\"][i]\n",
    "        ax_train.scatter(train_actual, train_pred, alpha=0.6, color=\"blue\", label=\"Training\")\n",
    "        ax_train.plot([min(train_actual), max(train_actual)],\n",
    "                      [min(train_actual), max(train_actual)],\n",
    "                      'r--', label=\"Ideal Fit\")\n",
    "        ax_train.set_xlabel(\"Actual Values\")\n",
    "        ax_train.set_ylabel(\"Predicted Values\")\n",
    "        ax_train.set_title(f\"Fold {i+1} Training\")\n",
    "        ax_train.legend()\n",
    "        # Correlation text\n",
    "        corr_val = all_correlations[i]\n",
    "        ax_train.text(\n",
    "            0.95, 0.05,\n",
    "            f\"Spearman: {corr_val['train_spearman']:.2f}\\nPearson: {corr_val['train_pearson']:.2f}\",\n",
    "            transform=ax_train.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "        )\n",
    "\n",
    "        # Validation subplot (bottom row)\n",
    "        ax_val = axes[1, i]\n",
    "        val_actual = fold_outputs[\"fold_val_actual\"][i]\n",
    "        val_pred = fold_outputs[\"fold_val_pred\"][i]\n",
    "        ax_val.scatter(val_actual, val_pred, alpha=0.6, color=\"orange\", label=\"Validation\")\n",
    "        ax_val.plot([min(val_actual), max(val_actual)],\n",
    "                    [min(val_actual), max(val_actual)],\n",
    "                    'r--', label=\"Ideal Fit\")\n",
    "        ax_val.set_xlabel(\"Actual Values\")\n",
    "        ax_val.set_ylabel(\"Predicted Values\")\n",
    "        ax_val.set_title(f\"Fold {i+1} Validation\")\n",
    "        ax_val.legend()\n",
    "        # Correlation text\n",
    "        ax_val.text(\n",
    "            0.95, 0.05,\n",
    "            f\"Spearman: {corr_val['val_spearman']:.2f}\\nPearson: {corr_val['val_pearson']:.2f}\",\n",
    "            transform=ax_val.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{filename}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x,y = batch\n",
    "            output = model(x)\n",
    "            all_actuals.extend(y.cpu().numpy())\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    return all_actuals, all_predictions\n",
    "\n",
    "def plot_prediction_results(actual, predicted, filename = None):\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.xlabel(\"actual\")\n",
    "    plt.ylabel(\"predicted\")\n",
    "    plt.title(\"Actual vs Predicted\")\n",
    "    plt.show()\n",
    "    if filename: plt.savefig(filename)\n",
    "\n",
    "def train(dataset, config):    \n",
    "    output_dir = config.output_dir\n",
    "    fold_outputs = {\"fold_train_actual\":[], \n",
    "                    \"fold_train_pred\":[],\n",
    "                     \"fold_val_actual\":[], \n",
    "                     \"fold_val_pred\":[]}\n",
    "    logs = {\"fold_actuals\":[],\n",
    "            \"fold_preds\":[],\n",
    "            \"fold_train_actuals\":[],\n",
    "            \"fold_train_preds\":[],\n",
    "            \"fold_train_losses\":[],\n",
    "            \"fold_val_losses\":[],\n",
    "            \"train_num_genes\":[],\n",
    "            \"all_train_labels\":[],\n",
    "            \"val_num_genes\":[],\n",
    "            \"all_val_labels\":[],\n",
    "            \"f1_accuracy\":{},\n",
    "            \"all_correlation\":[]}\n",
    "    loss_fn = nn.MSELoss()\n",
    "    training_logs = {\"train_actual\" : [],\n",
    "                    \"train_pred\" : [],\n",
    "                    \"val_actual\" : [],\n",
    "                    \"val_pred\" : [],\n",
    "                    \"epoch_train_losses\" : [],\n",
    "                    \"epoch_val_losses\" : []}\n",
    "    model_output_dir = output_dir + \"/model/\"\n",
    "    if not os.path.exists(model_output_dir): os.makedirs(model_output_dir)\n",
    "\n",
    "    losses = {\"train_losses\":[], \"val_losses\":[]}\n",
    "    all_correlations = []\n",
    "    #### Training loop ##### \n",
    "\n",
    "    # kfold split\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(dataset)))):\n",
    "        model = MLP(config)\n",
    "        train_dataloader = DataLoader(Subset(dataset, train_idx), batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "        val_dataloader = DataLoader(Subset(dataset, val_idx), batch_size=config.batch_size, shuffle=False, num_workers=0)\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "        val_labels = []\n",
    "        # for fold in range(num_folds):\n",
    "        filename = fold\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "        if config.early_stopping:\n",
    "            early_stopping =  EarlyStopping(patience=config.patience, delta=config.delta)\n",
    "        print(f'Fold {fold+1}/{config.num_folds}')\n",
    "        log_memory_usage()\n",
    "\n",
    "        for epoch in tqdm(range(config.max_epochs)):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            for batch in train_dataloader:\n",
    "                x, y = batch\n",
    "                x = x.to(config.device)\n",
    "                y = y.to(config.device)\n",
    "                optimizer.zero_grad()\n",
    "                device = next(model.parameters()).device  \n",
    "                output = model(x)\n",
    "                output.requires_grad_(True)  \n",
    "                loss = loss_fn(output.float(), y.float())\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    x, y = batch\n",
    "                    x = x.to(config.device)\n",
    "                    y = y.to(config.device)\n",
    "                    output = model(x)\n",
    "                    loss = loss_fn(output.float(), y.float())                \n",
    "                    val_losses.append(loss.item())\n",
    "                epoch_train_losses.append(mean(train_losses))\n",
    "                epoch_val_losses.append(mean(val_losses))\n",
    "                if config.early_stopping:\n",
    "                    early_stopping(mean(val_losses), model)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        early_stopping.load_best_model(model)\n",
    "                        break\n",
    "        \n",
    "        logs[\"all_val_labels\"].append(val_labels)\n",
    "        logs[\"fold_train_losses\"].append(epoch_train_losses)\n",
    "        logs[\"fold_val_losses\"].append(epoch_val_losses)\n",
    "        train_actual, train_pred = get_predictions(model, train_dataloader)\n",
    "        val_actual, val_pred = get_predictions(model, val_dataloader)\n",
    "        logs[\"fold_actuals\"].append(val_actual)\n",
    "        logs[\"fold_preds\"].append(val_pred)\n",
    "        logs[\"fold_train_actuals\"].append(train_actual)\n",
    "        logs[\"fold_train_preds\"].append(train_pred)\n",
    "        correlations = {\n",
    "            \"train_spearman\": compute_spearman_correlation(train_actual, train_pred) or 0,\n",
    "            \"val_spearman\": compute_spearman_correlation(val_actual, val_pred) or 0,\n",
    "            \"train_pearson\": compute_pearson_correlation(train_actual, train_pred) or 0,\n",
    "            \"val_pearson\": compute_pearson_correlation(val_actual, val_pred) or 0\n",
    "        }\n",
    "        all_correlations.append(correlations)\n",
    "        logs[\"all_correlation\"].append(correlations)\n",
    "        # plot_epoch_losses(epoch_train_losses, epoch_val_losses, filename, output_dir)\n",
    "        # plot_fold_scatter(val_actual, val_pred, train_actual, train_pred, correlations, fold, output_dir)        \n",
    "\n",
    "        losses[\"train_losses\"].append(epoch_train_losses), losses[\"val_losses\"].append(epoch_val_losses)\n",
    "        fold_outputs[\"fold_train_actual\"].append(train_actual), fold_outputs[\"fold_train_pred\"].append(train_pred), fold_outputs[\"fold_val_actual\"].append(val_actual), fold_outputs[\"fold_val_pred\"].append(val_pred), \n",
    "        log_memory_usage()\n",
    "        torch.save(model.state_dict(), os.path.join(model_output_dir, f\"model_fold_{fold}.pth\"))\n",
    "        # plot_correlations(logs[\"all_correlation\"], output_dir)\n",
    "\n",
    "    # Save logs and fold_outputs as pkl\n",
    "    plot_all_folds_losses(losses, output_dir, filename=\"all_folds\")\n",
    "    plot_all_folds_scatter(fold_outputs, all_correlations, output_dir, filename=\"all_folds_scatter\")\n",
    "\n",
    "    logs_file = os.path.join(output_dir, \"logs.pkl\")\n",
    "    with open(logs_file, \"wb\") as f:\n",
    "        pickle.dump(logs, f)\n",
    "    print(f\"Logs outputs saved to {logs_file}\")\n",
    "    fold_outputs_file = os.path.join(output_dir, \"fold_outputs.pkl\")\n",
    "    with open(fold_outputs_file, \"wb\") as f:\n",
    "        pickle.dump(fold_outputs, f)\n",
    "    print(f\"Fold outputs saved to {fold_outputs_file}\")\n",
    "\n",
    "    return model, logs, fold_outputs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd9d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/z_zb6snx1n95mjp1hndhb_km0000gn/T/ipykernel_9159/249355261.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq_data = torch.tensor(seq_data.reshape(seq_data.shape[0], -1), dtype=torch.float32).to(config.device)\n",
      "/var/folders/sq/z_zb6snx1n95mjp1hndhb_km0000gn/T/ipykernel_9159/249355261.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels   = torch.tensor(labels, dtype=torch.float32).to(config.device)\n",
      "/var/folders/sq/z_zb6snx1n95mjp1hndhb_km0000gn/T/ipykernel_9159/1611817260.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x, dtype=torch.float32)\n",
      "/var/folders/sq/z_zb6snx1n95mjp1hndhb_km0000gn/T/ipykernel_9159/1611817260.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Memory Usage: 98.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:40<01:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 136.86 MB\n",
      "Fold 2/5\n",
      "Memory Usage: 143.36 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:40<01:34,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 144.91 MB\n",
      "Fold 3/5\n",
      "Memory Usage: 147.48 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:40<01:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 142.53 MB\n",
      "Fold 4/5\n",
      "Memory Usage: 145.12 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:40<01:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 118.77 MB\n",
      "Fold 5/5\n",
      "Memory Usage: 123.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:41<01:37,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 110.41 MB\n",
      "Logs outputs saved to /Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli kappel/results/seed_0/lr_0.001/hidden_dim_512/num_layers_3/batch_size_10/early_stopping_patience_30_delta_0.05/logs.pkl\n",
      "Fold outputs saved to /Users/clairehsieh/OneDrive/Documents/UCLA/rotations/kalli kappel/results/seed_0/lr_0.001/hidden_dim_512/num_layers_3/batch_size_10/early_stopping_patience_30_delta_0.05/fold_outputs.pkl\n"
     ]
    }
   ],
   "source": [
    "seq_data = torch.tensor(seq_data.reshape(seq_data.shape[0], -1), dtype=torch.float32).to(config.device)\n",
    "labels   = torch.tensor(labels, dtype=torch.float32).to(config.device)\n",
    "dataset = CondensateDataset(seq_data, labels)\n",
    "\n",
    "model = MLP(config)\n",
    "model, logs, fold_outputs, losses = train(dataset, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f804012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "556ab469",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- First, a set of 79 “base sequences” was selected for extensive\n",
    "mutagenesis. These sequences include 43 of the fragments of natural protein sequences\n",
    "and one designed variant from the small sequence library, for a total of 44 “Class 1 base\n",
    "sequences.” Another 35 fragments of natural protein sequences (“Class 2 base\n",
    "sequences”) were further selected from PhasePro, excluding protein sequences that were\n",
    "annotated as partner dependent. To this end, all protein regions annotated to drive phase\n",
    "separation were examined, and all possible 66 amino acid fragments with amino acid\n",
    "composition and dipeptide composition similarity (Pearson correlation coefficient (r2)) to\n",
    "the Class 1 base sequences and to each other of less than 0.6 (“Class 3 sequences”)\n",
    "were identified. 35 of these sequences, including at most one sequence fragment per\n",
    "protein and prioritizing sequence fragments with the highest amino acid composition\n",
    "similarity to the full protein region as well as regions that were predicted to be more\n",
    "disordered.\n",
    "\n",
    "- The natural protein sequence fragment set includes: (1) all base sequences, (2) all\n",
    "remaining Class 3 sequences (283 sequences), (3) Class 4A sequences (519\n",
    "sequences): fragments from disordered (as annotated by MobiDB36) sequences from\n",
    "LLPSDB37 that were annotated either as phase separating or not phase separating with\n",
    "maximum amino acid composition and dipeptide correlation of 0.8 to each other and to\n",
    "all base sequences and all Class 3 sequences; and (4) Class 4B sequences (798\n",
    "sequences): disordered regions (as annotated by MobiDB) from Disprot38 (release\n",
    "2022_03) with maximum amino acid composition and dipeptide correlation of 0.6 to each\n",
    "other and to all base sequences, Class 3 sequences, and Class 4A sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babae745",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
